**List od kierowników projektu**

Lista OWASP Top 10 dla dużych modeli językowych została stworzona w 2023
r. jako wspólna inicjatywa społeczności mająca na celu zwrócenie uwagi
na problemy bezpieczeństwa charakterystyczne dla aplikacji AI i zajęcie
się nimi. Od tego czasu technologia ta nadal rozprzestrzenia się w
różnych branżach i aplikacjach, a wraz z nią rosną związane z nią
zagrożenia. W miarę jak duże modele językowe stają się coraz bardziej
integralną częścią wszystkich procesów, od interakcji z klientami po
operacje wewnętrzne, programiści i specjaliści ds. bezpieczeństwa
odkrywają nowe luki w zabezpieczeniach --- oraz sposoby ich
eliminowania.

Lista z 2023 roku odniosła duży sukces w podnoszeniu świadomości i
tworzeniu podstaw dla bezpiecznego korzystania z LLM, ale od tego czasu
nauczyliśmy się jeszcze więcej. W nowej wersji z 2025 roku
współpracowaliśmy z większą, bardziej zróżnicowaną grupą
współpracowników z całego świata, którzy pomogli w stworzeniu tej listy.
Proces ten obejmował burze mózgów, głosowanie i opinie praktyków
zajmujących się bezpieczeństwem aplikacji LLM, którzy dzielili się
swoimi doświadczeniami lub udoskonalali pozycje na liście. Każda opinia
miała kluczowe znaczenie dla tego, aby nowa wersja była jak najbardziej
kompletna i praktyczna.

**Co nowego w liście 10 największych zagrożeń na rok 2025**

Lista na rok 2025 odzwierciedla lepsze zrozumienie istniejących zagrożeń
i zawiera istotne aktualizacje dotyczące sposobu wykorzystania modeli
LLM w rzeczywistych aplikacjach. Na przykład pozycja **Nieograniczone
zużycie** rozszerza poprzednią pozycję Odmowa usługi o ryzyko związane z
zarządzaniem zasobami i nieoczekiwanymi kosztami --- pilną kwestią w
przypadku wdrożeń LLM na dużą skalę.

Pozycja **Wektory i osadzanie** stanowi odpowiedź na prośby społeczności
o wytyczne dotyczące zabezpieczania generowania rozszerzonego o
odzyskiwanie (RAG) i innych metod opartych na osadzaniu, które są
obecnie podstawowymi praktykami w zakresie ugruntowywania wyników
modeli.

Dodaliśmy również **Wyciek monitu systemowego**, aby zająć się obszarem
rzeczywistych exploitów, o który bardzo prosiła społeczność. Wiele
aplikacji zakładało, że polecenia są bezpiecznie izolowane, ale ostatnie
incydenty pokazały, że programiści nie mogą bezpiecznie zakładać, że
informacje zawarte w tych poleceniach pozostają tajne.

Rozszerzono pozycję **Nadmierna sprawczość**, biorąc pod uwagę coraz
częstsze stosowanie architektur agencyjnych, które mogą zapewnić modelom
LLM większą autonomię. Ponieważ modele LLM działają jako agenci lub w
ustawieniach wtyczek, niekontrolowane uprawnienia mogą prowadzić do
niezamierzonych lub ryzykownych działań, co sprawia, że pozycja ta jest
ważniejsza niż kiedykolwiek.

**Kolejne kroki**

Podobnie jak sama technologia, lista ta jest wynikiem spostrzeżeń i
doświadczeń społeczności open source.

Została ona ukształtowana dzięki wkładowi programistów, naukowców
zajmujących się danymi i ekspertów ds. bezpieczeństwa z różnych
sektorów, którzy są zaangażowani w tworzenie bezpieczniejszych aplikacji
AI. Z dumą przedstawiamy Państwu wersję na rok 2025 i mamy nadzieję, że
dostarczy ona Państwu narzędzi i wiedzy niezbędnych do skutecznego
zabezpieczenia modeli LLM.

Dziękujemy wszystkim, którzy pomogli w stworzeniu tego dokumentu, oraz
tym, którzy nadal z niego korzystają i ulepszają go. Jesteśmy wdzięczni,
że możemy uczestniczyć w tej pracy wraz z Państwem.

**Steve Wilson**

Kierownik projektu OWASP Top 10 dla aplikacji wykorzystujących duże
modele językowe LinkedIn:
[[https://www.linkedin.com/in/wilsonsd/]{.underline}](https://www.linkedin.com/in/wilsonsd/)

**Ads Dawson**

Kierownik techniczny i kierownik ds. wpisów dotyczących luk w
zabezpieczeniach OWASP Top 10 dla aplikacji wykorzystujących duże modele
językowe LinkedIn:
[[https://www.linkedin.com/in/adamdawson0/]{.underline}](https://www.linkedin.com/in/adamdawson0/)
